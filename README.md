# Task-1-PySpark-Data-Processing

# PySpark Data Processing

This repository contains PySpark code for data processing tasks, specifically focusing on reading, displaying, and saving data in different formats.

## Code Overview

- **Spark Session Initialization**: Initializes a Spark session named "example".
- **Data Loading**: Reads a CSV file into a Spark DataFrame.
- **Data Display**: Displays the first 20 rows and the schema of the DataFrame.
- **Data Saving**: Saves the DataFrame in Parquet and Delta formats.

## Usage Instructions

1. **Setup Spark Session**: Initialize a Spark session in your script.
2. **Data Source**: Place your CSV file in the appropriate location.
3. **Run Script**: Execute the script to process and save data.
4. **Check Outputs**: Verify the saved files.

## Prerequisites

- Apache Spark (PySpark)
- Delta Lake (for Delta format)
- Access to Databricks or compatible environment

## Notes

- Ensure paths used for saving files are accessible.
